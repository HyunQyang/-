{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20개 카테고리 전체 목록:\n",
      "['alt.atheism', 'comp.graphics', 'comp.os.ms-windows.misc', 'comp.sys.ibm.pc.hardware', 'comp.sys.mac.hardware', 'comp.windows.x', 'misc.forsale', 'rec.autos', 'rec.motorcycles', 'rec.sport.baseball', 'rec.sport.hockey', 'sci.crypt', 'sci.electronics', 'sci.med', 'sci.space', 'soc.religion.christian', 'talk.politics.guns', 'talk.politics.mideast', 'talk.politics.misc', 'talk.religion.misc']\n",
      "\n",
      "\n",
      "샘플 이메일\n",
      "From: lerxst@wam.umd.edu (where's my thing)\n",
      "Subject: WHAT car is this!?\n",
      "Nntp-Posting-Host: rac3.wam.umd.edu\n",
      "Organization: University of Maryland, College Park\n",
      "Lines: 15\n",
      "\n",
      " I was wondering if anyone out there could enlighten me on this car I saw\n",
      "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
      "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
      "the front bumper was separate from the rest of the body. This is \n",
      "all I know. If anyone can tellme a model name, engine specs, years\n",
      "of production, where this car is made, history, or whatever info you\n",
      "have on this funky looking car, please e-mail.\n",
      "\n",
      "Thanks,\n",
      "- IL\n",
      "   ---- brought to you by your neighborhood Lerxst ----\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "샘플 타깃 카테고리:\n",
      "7\n",
      "rec.autos\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "newsgroups_train =  fetch_20newsgroups(subset = 'train')\n",
    "newsgroups_test =  fetch_20newsgroups(subset = 'test')\n",
    "\n",
    "x_train = newsgroups_train.data\n",
    "x_test = newsgroups_test.data\n",
    "\n",
    "y_train = newsgroups_train.target\n",
    "y_test = newsgroups_test.target\n",
    "\n",
    "print(\"20개 카테고리 전체 목록:\")\n",
    "print(newsgroups_train.target_names)\n",
    "print(\"\\n\")\n",
    "print(\"샘플 이메일\")\n",
    "print(x_train[0])\n",
    "print(\"샘플 타깃 카테고리:\")\n",
    "print(y_train[0])\n",
    "print(newsgroups_train.target_names[y_train[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import string\n",
    "import pandas as pd\n",
    "from nltk import pos_tag\n",
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing(text):\n",
    "    text2 = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in text]).split())\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text2) for word in nltk.word_tokenize(sent)]\n",
    "    tokens = [word.lower() for word in tokens]\n",
    "    \n",
    "    stopwds = stopwords.words('english')\n",
    "    tokens = [token for token in tokens if token not in stopwds]\n",
    "    tokens = [word for word in tokens if len(word)>=3]\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    tagged_corpus = pos_tag(tokens)\n",
    "    \n",
    "    Noun_tags = ['NN','NNP','NNPS','NNS']\n",
    "    Verb_tags = ['VB','VBD','VBG','VBN','VBP','VBZ']\n",
    "    \n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    def part_lemmatize(token, tag):\n",
    "        if tag in Noun_tags:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        elif tag in Verb_tags:\n",
    "            return lemmatizer.lemmatize(token,'v')\n",
    "        else:\n",
    "            return lemmatizer.lemmatize(token,'n')\n",
    "        \n",
    "    pre_proc_text = \" \".join([part_lemmatize(token,tag) for token, tag in tagged_corpus])\n",
    "    \n",
    "    return pre_proc_text\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_preprocessed = []\n",
    "for i in x_train:\n",
    "    x_train_preprocessed.append(preprocessing(i))\n",
    "\n",
    "x_test_preprocessed = []\n",
    "for i in x_test:\n",
    "    x_test_preprocessed.append(preprocessing(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(min_df=2, ngram_range=(1, 2),  stop_words='english', \n",
    "                             max_features= 10000,strip_accents='unicode',  norm='l2')\n",
    "\n",
    "x_train_2 = vectorizer.fit_transform(x_train_preprocessed).todense()\n",
    "x_test_2 = vectorizer.transform(x_test_preprocessed).todense()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deep learning module\n",
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import Adadelta,Adam,RMSprop\n",
    "from keras.utils import np_utils\n",
    "\n",
    "np.random.seed(1337) #랜덤 시드 고정\n",
    "nb_classes = 20\n",
    "batch_size = 64\n",
    "nb_epochs = 20\n",
    "\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_4 (Dense)             (None, 1000)              10001000  \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 1000)              0         \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1000)              0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 500)               500500    \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 500)               0         \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 500)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 50)                25050     \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 50)                0         \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 20)                1020      \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 20)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 10,527,570\n",
      "Trainable params: 10,527,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential() # 순차 모델 정의\n",
    "\n",
    "model.add(Dense(1000,input_shape= (10000,))) # 출력된 뉴련의 수, 입력 차원\n",
    "model.add(Activation('relu')) \n",
    "model.add(Dropout(0.5)) # 노드를 줄여가는 비율\n",
    "\n",
    "model.add(Dense(500))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(50))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Dense(nb_classes))\n",
    "model.add(Activation('softmax')) #출력층\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam') # 손실 함수와 최적화\n",
    "\n",
    "print (model.summary()) # 모델의 구조 확인\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "177/177 [==============================] - 7s 36ms/step - loss: 1.9468\n",
      "Epoch 2/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.5846\n",
      "Epoch 3/20\n",
      "177/177 [==============================] - 6s 36ms/step - loss: 0.3033\n",
      "Epoch 4/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.1759\n",
      "Epoch 5/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.1211\n",
      "Epoch 6/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0925\n",
      "Epoch 7/20\n",
      "177/177 [==============================] - 6s 36ms/step - loss: 0.0618\n",
      "Epoch 8/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0540\n",
      "Epoch 9/20\n",
      "177/177 [==============================] - 6s 36ms/step - loss: 0.0472\n",
      "Epoch 10/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0387\n",
      "Epoch 11/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0382\n",
      "Epoch 12/20\n",
      "177/177 [==============================] - 6s 36ms/step - loss: 0.0364\n",
      "Epoch 13/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0332\n",
      "Epoch 14/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0303\n",
      "Epoch 15/20\n",
      "177/177 [==============================] - 6s 36ms/step - loss: 0.0264\n",
      "Epoch 16/20\n",
      "177/177 [==============================] - 6s 36ms/step - loss: 0.0248\n",
      "Epoch 17/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0256\n",
      "Epoch 18/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0215\n",
      "Epoch 19/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0227\n",
      "Epoch 20/20\n",
      "177/177 [==============================] - 6s 35ms/step - loss: 0.0253\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1dfc7533670>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  모델 학습 과정\n",
    "model.fit(x_train_2, Y_train, batch_size=batch_size, epochs=nb_epochs,verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_predclass = np.argmax(model.predict(x_train_2),axis=1)\n",
    "y_test_predclass = np.argmax(model.predict(x_test_2), axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8], dtype=int64)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_predclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4,  9,  0, ...,  9, 12, 15], dtype=int64)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test_predclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 4, 4, ..., 3, 1, 8])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 7,  5,  0, ...,  9,  6, 15])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Deep Neural Network - Train accuracy:  0.999\n",
      "\n",
      "Deep Neural Network - Test accuracy: 0.807\n",
      "\n",
      "Deep Neural Network - Train Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       480\n",
      "           1       1.00      1.00      1.00       584\n",
      "           2       1.00      1.00      1.00       591\n",
      "           3       1.00      1.00      1.00       590\n",
      "           4       1.00      1.00      1.00       578\n",
      "           5       1.00      1.00      1.00       593\n",
      "           6       0.99      1.00      1.00       585\n",
      "           7       1.00      1.00      1.00       594\n",
      "           8       1.00      1.00      1.00       598\n",
      "           9       1.00      1.00      1.00       597\n",
      "          10       1.00      1.00      1.00       600\n",
      "          11       1.00      1.00      1.00       595\n",
      "          12       1.00      0.99      1.00       591\n",
      "          13       1.00      1.00      1.00       594\n",
      "          14       1.00      1.00      1.00       593\n",
      "          15       1.00      1.00      1.00       599\n",
      "          16       1.00      1.00      1.00       546\n",
      "          17       1.00      1.00      1.00       564\n",
      "          18       1.00      1.00      1.00       465\n",
      "          19       1.00      1.00      1.00       377\n",
      "\n",
      "    accuracy                           1.00     11314\n",
      "   macro avg       1.00      1.00      1.00     11314\n",
      "weighted avg       1.00      1.00      1.00     11314\n",
      "\n",
      "\n",
      "Deep Neural Network - Test Classification Report\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.85      0.71      0.77       319\n",
      "           1       0.76      0.69      0.72       389\n",
      "           2       0.71      0.72      0.71       394\n",
      "           3       0.60      0.72      0.65       392\n",
      "           4       0.74      0.81      0.77       385\n",
      "           5       0.87      0.75      0.80       395\n",
      "           6       0.79      0.82      0.81       390\n",
      "           7       0.91      0.81      0.85       396\n",
      "           8       0.93      0.93      0.93       398\n",
      "           9       0.80      0.95      0.87       397\n",
      "          10       0.96      0.95      0.96       399\n",
      "          11       0.97      0.87      0.92       396\n",
      "          12       0.70      0.68      0.69       393\n",
      "          13       0.83      0.84      0.84       396\n",
      "          14       0.86      0.91      0.89       394\n",
      "          15       0.91      0.85      0.88       398\n",
      "          16       0.71      0.90      0.79       364\n",
      "          17       0.95      0.87      0.91       376\n",
      "          18       0.75      0.60      0.66       310\n",
      "          19       0.62      0.66      0.64       251\n",
      "\n",
      "    accuracy                           0.81      7532\n",
      "   macro avg       0.81      0.80      0.80      7532\n",
      "weighted avg       0.81      0.81      0.81      7532\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "print(('\\n\\nDeep Neural Network - Train accuracy: '),(round(accuracy_score(y_train,y_train_predclass),3)))\n",
    "print(('\\nDeep Neural Network - Test accuracy:'),(round(accuracy_score(y_test,y_test_predclass),3)))\n",
    "\n",
    "print('\\nDeep Neural Network - Train Classification Report')\n",
    "print(classification_report(y_train,y_train_predclass))\n",
    "\n",
    "print('\\nDeep Neural Network - Test Classification Report')\n",
    "print(classification_report(y_test,y_test_predclass))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
